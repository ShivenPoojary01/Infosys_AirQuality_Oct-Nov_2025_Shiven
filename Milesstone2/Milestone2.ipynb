{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618fa2dd-7997-47a8-a116-4c8b6ce1f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import All Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data processing and evaluation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Time Series Models\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Machine Learning Model\n",
    "import xgboost as xgb\n",
    "\n",
    "# Deep Learning Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Set display options for better visibility\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"âœ… All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e87fe2-1ddd-48fc-8a8d-6167bd4963a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeNoVo\\AppData\\Local\\Temp\\ipykernel_16476\\3982097164.py:7: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data.csv',encoding='latin-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying Status Column Creation ---\n",
      "Value counts for AQ_Status column before aggregation:\n",
      "AQ_Status\n",
      "Moderate        191739\n",
      "Satisfactory    144771\n",
      "Good             79985\n",
      "Poor             14689\n",
      "Very Poor         2936\n",
      "Severe            1622\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "\n",
      "âœ… Human-readable file saved: 'human_readable_air_quality.csv'\n",
      "Head of the final human-readable file:\n",
      "        date          state  avg_rspm  avg_so2  avg_no2 Air_Quality_Status\n",
      "0 1987-01-01        Gujarat    108.83    19.70    15.00           Moderate\n",
      "1 1987-01-01      Rajasthan    108.83    10.83    25.81           Moderate\n",
      "2 1987-01-01  Uttar Pradesh    108.83    16.10    25.81           Moderate\n",
      "3 1987-01-01    West Bengal    108.83    47.04    86.37           Moderate\n",
      "4 1987-01-02        Gujarat    108.83    26.45    45.40           Moderate\n",
      "\n",
      "âœ… Model-ready file for Maharashtra saved: 'clean_air_quality_timeseries.csv'\n",
      "\n",
      "Data is ready for model training.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 (Corrected for Status Column): Data Cleaning and File Creation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data.csv',encoding='latin-1')\n",
    "\n",
    "# --- 2.1 Cleaning and Type Conversion ---\n",
    "pollutant_cols = ['so2', 'no2', 'rspm', 'spm', 'pm2_5']\n",
    "for col in pollutant_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.drop(columns=['stn_code', 'sampling_date', 'agency', 'location_monitoring_station'], errors='ignore')\n",
    "\n",
    "# --- 2.2 Imputation ---\n",
    "# Fill missing pollutant values with the mean of the column\n",
    "for col in pollutant_cols:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "# --- 2.3 Feature Engineering: Air Quality Status Column ---\n",
    "def get_aq_status(rspm):\n",
    "    if not pd.isna(rspm):\n",
    "        if rspm <= 50: return 'Good'\n",
    "        elif rspm <= 100: return 'Satisfactory'\n",
    "        elif rspm <= 250: return 'Moderate'\n",
    "        elif rspm <= 350: return 'Poor'\n",
    "        elif rspm <= 430: return 'Very Poor'\n",
    "        else: return 'Severe'\n",
    "    return 'Unknown' # Return 'Unknown' if rspm is still NaN for some reason\n",
    "\n",
    "# Create the status column\n",
    "df['AQ_Status'] = df['rspm'].apply(get_aq_status)\n",
    "\n",
    "# --- Verification Step (for you to see it's working) ---\n",
    "print(\"--- Verifying Status Column Creation ---\")\n",
    "print(\"Value counts for AQ_Status column before aggregation:\")\n",
    "print(df['AQ_Status'].value_counts())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 2.4 Aggregation (Per-State Model) ---\n",
    "# A more robust function to find the most common status\n",
    "def get_most_common_status(series):\n",
    "    if not series.mode().empty:\n",
    "        return series.mode().iloc[0]\n",
    "    return 'Unknown'\n",
    "\n",
    "# Aggregate data by date and state\n",
    "agg_df = df.groupby(['date', 'state']).agg(\n",
    "    avg_rspm=('rspm', 'mean'),\n",
    "    avg_so2=('so2', 'mean'),\n",
    "    avg_no2=('no2', 'mean'),\n",
    "    Air_Quality_Status=('AQ_Status', get_most_common_status) # Use the new robust function\n",
    ").reset_index()\n",
    "\n",
    "# Round the averages for readability\n",
    "agg_df['avg_rspm'] = agg_df['avg_rspm'].round(2)\n",
    "agg_df['avg_so2'] = agg_df['avg_so2'].round(2)\n",
    "agg_df['avg_no2'] = agg_df['avg_no2'].round(2)\n",
    "\n",
    "# --- 2.5 SAVE THE HUMAN-READABLE CSV (YOUR REQUEST) ---\n",
    "agg_df.to_csv('human_readable_air_quality.csv', index=False)\n",
    "print(\"\\nâœ… Human-readable file saved: 'human_readable_air_quality.csv'\")\n",
    "print(\"Head of the final human-readable file:\")\n",
    "print(agg_df.head())\n",
    "\n",
    "\n",
    "# --- 2.6 PREPARE DATA FOR MODELS (STILL REQUIRED) ---\n",
    "STATE_TO_PREDICT = 'Maharashtra'\n",
    "ts_state_df = agg_df[agg_df['state'] == STATE_TO_PREDICT]\n",
    "\n",
    "final_ts_df = ts_state_df[['date', 'avg_rspm']].rename(\n",
    "    columns={'date': 'ds', 'avg_rspm': 'y'}\n",
    ")\n",
    "\n",
    "final_ts_df.to_csv('clean_air_quality_timeseries.csv', index=False)\n",
    "print(f\"\\nâœ… Model-ready file for {STATE_TO_PREDICT} saved: 'clean_air_quality_timeseries.csv'\")\n",
    "\n",
    "# --- 2.7 Split the data for the next steps ---\n",
    "TEST_SIZE = 30\n",
    "train_df = final_ts_df.iloc[:-TEST_SIZE].copy()\n",
    "test_df = final_ts_df.iloc[-TEST_SIZE:].copy()\n",
    "y_actual = test_df['y'].values\n",
    "model_results = {} # Reset results for the new run\n",
    "\n",
    "print(f\"\\nData is ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e06bf1-ccf2-4148-9c86-f5dfbcb19f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training ARIMA Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeNoVo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\LeNoVo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\LeNoVo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\LeNoVo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\LeNoVo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ARIMA - MAE: 7.52, RMSE: 9.33\n",
      "\n",
      "--- Training Prophet Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:53:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:53:27 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prophet - MAE: 7.89, RMSE: 10.01\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Train and Evaluate ARIMA and Prophet Models\n",
    "\n",
    "# --- 3.1 ARIMA Model ---\n",
    "print(\"\\n--- Training ARIMA Model ---\")\n",
    "arima_train_series = train_df.set_index('ds')['y']\n",
    "try:\n",
    "    m_arima = ARIMA(arima_train_series, order=(5, 1, 0)).fit()\n",
    "    y_pred_arima = m_arima.forecast(steps=TEST_SIZE).values\n",
    "    mae_arima = mean_absolute_error(y_actual, y_pred_arima)\n",
    "    rmse_arima = np.sqrt(mean_squared_error(y_actual, y_pred_arima))\n",
    "    model_results['ARIMA'] = {'MAE': mae_arima, 'RMSE': rmse_arima, 'Model': m_arima}\n",
    "    print(f\"âœ… ARIMA - MAE: {mae_arima:.2f}, RMSE: {rmse_arima:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ARIMA failed: {e}\")\n",
    "    model_results['ARIMA'] = {'MAE': np.inf, 'RMSE': np.inf, 'Model': None}\n",
    "\n",
    "# --- 3.2 Prophet Model ---\n",
    "print(\"\\n--- Training Prophet Model ---\")\n",
    "m_prophet = Prophet(daily_seasonality=True, weekly_seasonality=True)\n",
    "m_prophet.fit(train_df)\n",
    "y_pred_prophet = m_prophet.predict(test_df[['ds']])['yhat'].values\n",
    "mae_prophet = mean_absolute_error(y_actual, y_pred_prophet)\n",
    "rmse_prophet = np.sqrt(mean_squared_error(y_actual, y_pred_prophet))\n",
    "model_results['Prophet'] = {'MAE': mae_prophet, 'RMSE': rmse_prophet, 'Model': m_prophet}\n",
    "print(f\"âœ… Prophet - MAE: {mae_prophet:.2f}, RMSE: {rmse_prophet:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1725da52-1fec-4d0c-b54f-fd15f8cb1e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training XGBoost Model ---\n",
      "âœ… XGBoost - MAE: 12.23, RMSE: 16.13\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Train and Evaluate XGBoost Model\n",
    "\n",
    "print(\"\\n--- Training XGBoost Model ---\")\n",
    "\n",
    "# --- 4.1 Feature Engineering ---\n",
    "def create_features(df_in):\n",
    "    df_out = df_in.copy()\n",
    "    df_out['dayofweek'] = df_out['ds'].dt.dayofweek\n",
    "    df_out['month'] = df_out['ds'].dt.month\n",
    "    df_out['year'] = df_out['ds'].dt.year\n",
    "    df_out['lag_7'] = df_out['y'].shift(7)\n",
    "    return df_out.dropna()\n",
    "\n",
    "full_df_features = create_features(final_ts_df)\n",
    "train_features = full_df_features[full_df_features['ds'].isin(train_df['ds'])]\n",
    "test_features = full_df_features[full_df_features['ds'].isin(test_df['ds'])]\n",
    "FEATURES = ['dayofweek', 'month', 'year', 'lag_7']\n",
    "X_train, y_train = train_features[FEATURES], train_features['y']\n",
    "X_test, y_test = test_features[FEATURES], test_features['y']\n",
    "\n",
    "# --- 4.2 Training and Evaluation ---\n",
    "m_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "m_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = m_xgb.predict(X_test)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "model_results['XGBoost'] = {'MAE': mae_xgb, 'RMSE': rmse_xgb, 'Model': m_xgb}\n",
    "print(f\"âœ… XGBoost - MAE: {mae_xgb:.2f}, RMSE: {rmse_xgb:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2373b9d7-ec88-4e98-94ad-9867d75880f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LSTM Model (this will show progress) ---\n",
      "Epoch 1/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - loss: 0.0137 - val_loss: 0.0025\n",
      "Epoch 2/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - loss: 0.0090 - val_loss: 0.0014\n",
      "Epoch 3/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0089 - val_loss: 0.0015\n",
      "Epoch 4/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 0.0087 - val_loss: 0.0013\n",
      "Epoch 5/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - loss: 0.0085 - val_loss: 0.0020\n",
      "Epoch 6/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step - loss: 0.0082 - val_loss: 0.0018\n",
      "Epoch 7/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0081 - val_loss: 0.0011\n",
      "Epoch 8/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - loss: 0.0079 - val_loss: 0.0012\n",
      "Epoch 9/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - loss: 0.0076 - val_loss: 0.0013\n",
      "Epoch 10/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 0.0075 - val_loss: 0.0013\n",
      "Epoch 11/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - loss: 0.0074 - val_loss: 0.0015\n",
      "Epoch 12/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - loss: 0.0073 - val_loss: 0.0014\n",
      "Epoch 13/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - loss: 0.0074 - val_loss: 0.0014\n",
      "Epoch 14/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - loss: 0.0073 - val_loss: 0.0016\n",
      "Epoch 15/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 0.0074 - val_loss: 0.0013\n",
      "Epoch 16/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 99ms/step - loss: 0.0073 - val_loss: 0.0022\n",
      "Epoch 17/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 43ms/step - loss: 0.0072 - val_loss: 0.0016\n",
      "Epoch 18/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step - loss: 0.0073 - val_loss: 0.0015\n",
      "Epoch 19/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - loss: 0.0072 - val_loss: 0.0015\n",
      "Epoch 20/20\n",
      "\u001b[1m72/72\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - loss: 0.0072 - val_loss: 0.0033\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step\n",
      "âœ… LSTM - MAE: 12.69, RMSE: 15.29\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 (Corrected for ValueError): Train and Evaluate LSTM Model\n",
    "\n",
    "print(\"\\n--- Training LSTM Model (this will show progress) ---\")\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# --- 5.1 Data Preparation (with Corrected Sequence Function) ---\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(final_ts_df[['y']])\n",
    "train_data = scaled_data[:-TEST_SIZE]\n",
    "test_data = scaled_data[len(train_data) - 60:]\n",
    "\n",
    "# CORRECTED Function to create sequences of data\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    # The \"- 1\" was removed from the end of the range to create the correct number of samples\n",
    "    for i in range(len(dataset) - time_step):\n",
    "        a = dataset[i:(i + time_step), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "time_step = 60\n",
    "X_train_lstm, y_train_lstm = create_dataset(train_data, time_step)\n",
    "X_test_lstm, y_test_lstm = create_dataset(test_data, time_step)\n",
    "X_train_lstm = X_train_lstm.reshape(X_train_lstm.shape[0], X_train_lstm.shape[1], 1)\n",
    "X_test_lstm = X_test_lstm.reshape(X_test_lstm.shape[0], X_test_lstm.shape[1], 1)\n",
    "\n",
    "# --- 5.2 Build and Train Model ---\n",
    "m_lstm = Sequential([\n",
    "    Input(shape=(time_step, 1)),\n",
    "    LSTM(50, return_sequences=True),\n",
    "    LSTM(50),\n",
    "    Dense(1)\n",
    "])\n",
    "m_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "m_lstm.fit(X_train_lstm, y_train_lstm, validation_data=(X_test_lstm, y_test_lstm), \n",
    "           epochs=20, batch_size=64, verbose=1)\n",
    "\n",
    "# --- 5.3 Prediction and Evaluation ---\n",
    "y_pred_lstm_scaled = m_lstm.predict(X_test_lstm)\n",
    "y_pred_lstm = scaler.inverse_transform(y_pred_lstm_scaled)\n",
    "\n",
    "mae_lstm = mean_absolute_error(y_actual, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_actual, y_pred_lstm))\n",
    "model_results['LSTM'] = {'MAE': mae_lstm, 'RMSE': rmse_lstm, 'Model': m_lstm}\n",
    "print(f\"âœ… LSTM - MAE: {mae_lstm:.2f}, RMSE: {rmse_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224f209d-6833-4a61-b6dd-0af2a6860b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Performance Summary ---\n",
      "              MAE       RMSE\n",
      "ARIMA    7.524806    9.32817\n",
      "Prophet   7.89348  10.009985\n",
      "XGBoost  12.22904   16.12529\n",
      "LSTM     12.69202  15.288504\n",
      "\n",
      "ðŸŽ‰ Best performing model is 'ARIMA'.\n",
      "âœ… Model saved for inference as 'best_air_quality_model.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Final Model Comparison and Saving (Milestone 2 Completion)\n",
    "\n",
    "print(\"\\n--- Final Model Performance Summary ---\")\n",
    "\n",
    "# Create a DataFrame for easy comparison and sort by MAE\n",
    "performance_df = pd.DataFrame(model_results).T.drop(columns='Model').sort_values('MAE')\n",
    "print(performance_df)\n",
    "\n",
    "# --- Find and Save the Best Model (Instruction 4 & 5) ---\n",
    "best_model_name = performance_df.index[0]\n",
    "best_model_obj = model_results[best_model_name]['Model']\n",
    "\n",
    "# --- Save the best-performing model for inference ---\n",
    "if best_model_name == 'LSTM':\n",
    "    try:\n",
    "        # Keras native saving format is preferred\n",
    "        best_model_obj.save('best_air_quality_model.h5')\n",
    "        print(f\"\\nðŸŽ‰ Best performing model is '{best_model_name}'.\")\n",
    "        print(\"âœ… Model saved for inference as 'best_air_quality_model.h5'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCould not save Keras model directly. Error: {e}\")\n",
    "else:\n",
    "    # Use joblib for other models\n",
    "    joblib.dump(best_model_obj, 'best_air_quality_model.joblib')\n",
    "    print(f\"\\nðŸŽ‰ Best performing model is '{best_model_name}'.\")\n",
    "    print(\"âœ… Model saved for inference as 'best_air_quality_model.joblib'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81382f84-9420-428f-b7b3-d2a796b2c12a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
